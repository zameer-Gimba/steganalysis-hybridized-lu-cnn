# Hybrid LU Activation Function in CNN for Steganalysis

## Overview
This project explores a **dynamic hybridization of LU-based activation functions**
(ELU, ReLU, and LeakyReLU) within a Convolutional Neural Network (CNN) architecture
for **steganalysis**.

The work was developed during my **SIWES (Industrial Training)** at  
**NITDA ICT Hub, Federal University Kashere (FUK), Gombe State**,  
under the supervision of academic and technical staff.

The motivation is to improve feature sensitivity in steganalysis tasks, where
hidden information introduces subtle statistical deviations in image data.


## Key Idea
Instead of using a fixed activation function throughout the CNN, this approach:

- Observes activation output behavior
- Dynamically selects ELU, ReLU, or LeakyReLU
- Iteratively refines the model’s non-linearity

This hybrid strategy aims to enhance representational robustness for detecting
steganographic patterns.


## How to Run
1. Install Dependencies
```bash
pip install -r requirements.txt
```
2. Run the Model
```
python src/hybrid_lu_cnn.py
```
3. Run Tests
```
python src/hybrid_lu_cnn.py
```
## Results

Experimental visualizations such as activation distributions and architecture
comparisons are stored in the results/ directory.

### Activation Function Analysis & Impact

The figure above compares **ReLU**, **ELU**, and **Leaky ReLU** activation functions across negative and positive input regions.

**Key observations:**

* **ReLU** performs efficiently for positive inputs but completely suppresses negative values, which may lead to information loss (the *dying ReLU* problem).
* **ELU** preserves negative information smoothly, improving gradient flow and stability during training.
* **Leaky ReLU** maintains a small gradient for negative inputs, reducing neuron inactivity while remaining computationally simple.

**Hybrid Impact:**
By allowing these activation functions to **work together dynamically**, the hybrid LU strategy combines:

* ReLU’s efficiency on strong features,
* ELU’s stability for subtle negative signals,
* Leaky ReLU’s robustness against vanishing gradients.

This hybridization is particularly valuable in **steganalysis**, where hidden information

Note: Results are dataset-dependent and can be regenerated by extending the
current model with real steganalysis datasets.

## Evaluation Note
This project focuses on architectural optimization through a hybrid
activation-function strategy for CNN-based steganalysis.

Quantitative performance metrics (e.g., accuracy, F1-score) were not
reported because the work emphasizes activation behavior and model
adaptability rather than full dataset training.

## Academic Context

This project is part of my Machine Learning internship experience and is
included in my CV as applied research work in deep learning for information
security.

## License

This project is intended for educational and research purposes.
